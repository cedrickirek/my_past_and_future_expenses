# .github/workflows/ci.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-mock psutil
    
    - name: Create test data
      run: |
        cat > cedric_yearly_expenses_2024.csv << EOF
        Date,Category,Amount,Internship_period,Part_time_job_period
        01/09/2024,Rent,500,FALSE,TRUE
        01/09/2024,Subscriptions,2.99,FALSE,TRUE
        01/09/2024,Subscriptions,9.99,FALSE,TRUE
        03/09/2024,Groceries,29.26,FALSE,FALSE
        05/09/2024,Gifts-donations,50,FALSE,FALSE
        07/09/2024,Groceries,23.98,FALSE,FALSE
        EOF
    
    - name: Lint with flake8
      run: |
        pip install flake8
        # stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type checking with mypy
      run: |
        pip install mypy
        mypy . --ignore-missing-imports || true
    
    - name: Run unit tests
      run: |
        python -m pytest test_my_expenses.py -v --cov=my_expenses --cov-report=xml --cov-report=html
    
    - name: Upload coverage reports to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration-test:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.10
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest
    
    - name: Create test data
      run: |
        cat > cedric_yearly_expenses_2024.csv << EOF
        Date,Category,Amount,Internship_period,Part_time_job_period
        01/09/2024,Rent,500,FALSE,TRUE
        01/09/2024,Subscriptions,2.99,FALSE,TRUE
        03/09/2024,Groceries,29.26,FALSE,FALSE
        EOF
    
    - name: Test Streamlit app startup
      run: |
        # Test that the app can start without errors
        timeout 30s streamlit run my_expenses.py --server.headless=true || echo "App started successfully"
    
    - name: Run integration tests
      run: |
        python -c "
        import pandas as pd
        import numpy as np
        from my_expenses import load_data, get_season, train_prediction_model
        
        print('Testing data loading...')
        df = load_data()
        print(f'Loaded {len(df)} rows')
        
        print('Testing model training...')
        model, encoder = train_prediction_model(df)
        print('Model trained successfully')
        
        print('Testing prediction...')
        test_input = np.array([[9, 1, 0, encoder.transform(['Autumn'])[0]]])
        prediction = model.predict(test_input)
        print(f'Prediction: {prediction[0]:.2f}')
        print('Integration tests passed!')
        "

  docker-test:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Create test data
      run: |
        mkdir -p app
        cat > app/cedric_yearly_expenses_2024.csv << EOF
        Date,Category,Amount,Internship_period,Part_time_job_period
        01/09/2024,Rent,500,FALSE,TRUE
        01/09/2024,Subscriptions,2.99,FALSE,TRUE
        03/09/2024,Groceries,29.26,FALSE,FALSE
        EOF
    
    - name: Build Docker image
      run: |
        cd app
        docker build -t expenses-app .
    
    - name: Test Docker container
      run: |
        # Run container in background
        docker run -d -p 8501:8501 --name test-container expenses-app
        
        # Wait for container to start
        sleep 10
        
        # Test health endpoint
        curl -f http://localhost:8501/_stcore/health || echo "Health check endpoint not available"
        
        # Clean up
        docker stop test-container
        docker rm test-container

  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Bandit Security Lint
      run: |
        pip install bandit
        bandit -r . -f json -o bandit-report.json || true
    
    - name: Upload Bandit report
      uses: actions/upload-artifact@v3
      with:
        name: bandit-report
        path: bandit-report.json

  performance-test:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.10
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest psutil memory_profiler
    
    - name: Run performance tests
      run: |
        python -c "
        import time
        import psutil
        import pandas as pd
        import numpy as np
        from my_expenses import train_prediction_model, get_season
        
        # Create large dataset for performance testing
        print('Creating large dataset...')
        large_data = {
            'Date': pd.date_range('2020-01-01', periods=10000, freq='D'),
            'Amount': np.random.uniform(10, 500, 10000),
            'Category': np.random.choice(['Rent', 'Groceries', 'Transport'], 10000),
            'Internship_period': np.random.choice([True, False], 10000),
            'Part_time_job_period': np.random.choice([True, False], 10000)
        }
        df = pd.DataFrame(large_data)
        df['Month'] = df['Date'].dt.month
        df['Season'] = df['Date'].apply(lambda x: get_season(x.month))
        
        # Performance test
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        model, encoder = train_prediction_model(df)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        print(f'Training time: {end_time - start_time:.2f} seconds')
        print(f'Memory usage: {end_memory - start_memory:.2f} MB')
        
        # Assert reasonable performance
        assert end_time - start_time < 30, 'Training took too long'
        assert end_memory - start_memory < 500, 'Memory usage too high'
        print('Performance test passed!')
        "

  deploy-staging:
    runs-on: ubuntu-latest
    needs: [test, integration-test, docker-test]
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add your staging deployment commands here
        # For example, deploy to Heroku, AWS, or other cloud platforms
    
  deploy-production:
    runs-on: ubuntu-latest
    needs: [test, integration-test, docker-test, security-scan]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add your production deployment commands here
        # Ensure all tests pass before deploying to production

  notify:
    runs-on: ubuntu-latest
    needs: [test, integration-test, docker-test]
    if: always()
    
    steps:
    - name: Notify on success
      if: ${{ success() }}
      run: echo "✅ All tests passed successfully!"
    
    - name: Notify on failure
      if: ${{ failure() }}
      run: echo "❌ Some tests failed. Please check the logs."